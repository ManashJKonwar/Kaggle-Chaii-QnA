{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "! pip install 'ray[tune]'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR: Invalid requirement: \"'ray[tune]'\"\n",
      "WARNING: You are using pip version 20.2.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\manash.jyoti.konwar\\documents\\ai_kaggle_projects\\virtualenvs\\chaiiqnavenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "from datasets import Dataset, load_dataset, load_metric\r\n",
    "from transformers import AutoTokenizer\r\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\r\n",
    "from transformers import default_data_collator\r\n",
    "\r\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\r\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting Model Based Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "squad_v2 = False\r\n",
    "model_checkpoint_name = 'deepset/xlm-roberta-large-squad2'\r\n",
    "batch_size = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading Train, Test and Sample data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\r\n",
    "test = pd.read_csv(\"input/test.csv\")\r\n",
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the Tokenization Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\r\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\r\n",
    "\r\n",
    "def prepare_train_features(examples):\r\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\r\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\r\n",
    "    # left whitespace\r\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\r\n",
    "\r\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\r\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\r\n",
    "    # context that overlaps a bit the context of the previous feature.\r\n",
    "    tokenized_examples = tokenizer(\r\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\r\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\r\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\r\n",
    "        max_length=max_length,\r\n",
    "        stride=doc_stride,\r\n",
    "        return_overflowing_tokens=True,\r\n",
    "        return_offsets_mapping=True,\r\n",
    "        padding=\"max_length\",\r\n",
    "    )\r\n",
    "\r\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n",
    "    # its corresponding example. This key gives us just that.\r\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\r\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\r\n",
    "    # help us compute the start_positions and end_positions.\r\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\r\n",
    "\r\n",
    "    # Let's label those examples!\r\n",
    "    tokenized_examples[\"start_positions\"] = []\r\n",
    "    tokenized_examples[\"end_positions\"] = []\r\n",
    "\r\n",
    "    for i, offsets in enumerate(offset_mapping):\r\n",
    "        # We will label impossible answers with the index of the CLS token.\r\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\r\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\r\n",
    "\r\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\r\n",
    "\r\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\r\n",
    "        sample_index = sample_mapping[i]\r\n",
    "        answers = examples[\"answers\"][sample_index]\r\n",
    "        # If no answers are given, set the cls_index as answer.\r\n",
    "        if len(answers[\"answer_start\"]) == 0:\r\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\r\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\r\n",
    "        else:\r\n",
    "            # Start/end character index of the answer in the text.\r\n",
    "            start_char = answers[\"answer_start\"][0]\r\n",
    "            end_char = start_char + len(answers[\"text\"][0])\r\n",
    "\r\n",
    "            # Start token index of the current span in the text.\r\n",
    "            token_start_index = 0\r\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\r\n",
    "                token_start_index += 1\r\n",
    "\r\n",
    "            # End token index of the current span in the text.\r\n",
    "            token_end_index = len(input_ids) - 1\r\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\r\n",
    "                token_end_index -= 1\r\n",
    "\r\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\r\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\r\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\r\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\r\n",
    "            else:\r\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\r\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\r\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\r\n",
    "                    token_start_index += 1\r\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\r\n",
    "                while offsets[token_end_index][1] >= end_char:\r\n",
    "                    token_end_index -= 1\r\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\r\n",
    "\r\n",
    "    return tokenized_examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the train data to tokenize them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def preprocess_answers(answer_info):\r\n",
    "    ans_start_index = answer_info[0]\r\n",
    "    ans_text = answer_info[1]\r\n",
    "    return {\r\n",
    "        'answer_start': [ans_start_index],\r\n",
    "        'text': [ans_text]\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train = train.sample(frac=1, random_state=42)\r\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(preprocess_answers, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df_train = train[:-64].reset_index(drop=True)\r\n",
    "df_valid = train[-64:].reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\r\n",
    "valid_dataset = Dataset.from_pandas(df_valid)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\r\n",
    "tokenized_train_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)\r\n",
    "tokenized_valid_dataset = valid_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.91s/ba]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/ba]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyper Parameter Tuning by using Ray at backend and hyperopt as distributing platform"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# %env WANDB_DISABLED=True\r\n",
    "%env RAY_DISABLE_IMPORT_WARNING=1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: RAY_DISABLE_IMPORT_WARNING=1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model_name = model_checkpoint_name.split(\"/\")[-1]\r\n",
    "args = TrainingArguments(\r\n",
    "    f\"chaii-qa-xlmroberta-hyperopt\",\r\n",
    "    evaluation_strategy = \"epoch\",\r\n",
    "    save_strategy = \"epoch\",\r\n",
    "    learning_rate = 2e-5,\r\n",
    "    per_device_train_batch_size = batch_size,\r\n",
    "    per_device_eval_batch_size = batch_size,\r\n",
    "    warmup_ratio = 0.1,\r\n",
    "    gradient_accumulation_steps = 8,\r\n",
    "    num_train_epochs = 1,\r\n",
    "    weight_decay = 0.01,\r\n",
    "    report_to = \"wandb\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data_collator = default_data_collator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def model_init():\r\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(model_checkpoint_name, return_dict=True)\r\n",
    "\r\n",
    "def compute_metrics(eval_pred):\r\n",
    "    predictions, labels = eval_pred\r\n",
    "    predictions = predictions.argmax(axis=-1)\r\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "trainer = Trainer(\r\n",
    "    args=args,\r\n",
    "    train_dataset=tokenized_train_dataset,\r\n",
    "    eval_dataset=tokenized_valid_dataset,\r\n",
    "    data_collator=data_collator,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    model_init=model_init,\r\n",
    "    compute_metrics=compute_metrics\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/deepset/xlm-roberta-large-squad2/resolve/main/config.json from cache at C:\\Users\\manash.jyoti.konwar/.cache\\huggingface\\transformers\\531c1582e1ea0b7d34c7de10efd3593838f1018f8d012b8029c9283c41cba7c0.09d513aaf4fbccf6b8b4d0264d74ea7dc8d6fb056bdb099e45621b06d8c877de\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"name\": \"XLMRoberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/deepset/xlm-roberta-large-squad2/resolve/main/pytorch_model.bin from cache at C:\\Users\\manash.jyoti.konwar/.cache\\huggingface\\transformers\\884c0f33c3e77d0ae9767739e9a8d4e4bd7cad073cd0a77f39f9653a69cc0ab6.250ea9b0a06128f28a2553e930e58780d5e31f8f99bb8ef0ee7bf7996437ada6\n",
      "All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at deepset/xlm-roberta-large-squad2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Default objective is the sum of all metrics\r\n",
    "# when metrics are provided, so we have to maximize it.\r\n",
    "best_trial  = trainer.hyperparameter_search(\r\n",
    "    direction=\"maximize\", \r\n",
    "    backend=\"ray\", \r\n",
    "    #n_trials=10 # number of trials\r\n",
    "    # Choose among many libraries:\r\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\r\n",
    "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\r\n",
    "    # Choose among schedulers:\r\n",
    "    # https://docs.ray.io/en/latest/tune/api_docs/schedulers.html\r\n",
    "    scheduler=ASHAScheduler(metric=\"objective\", mode=\"max\")\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No `resources_per_trial` arg was passed into `hyperparameter_search`. Setting it to a default value of 1 CPU for each trial.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_trial.__params__"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Kernel is dead",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:1006195)",
      "at g.sendShellMessage (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:1005964)",
      "at g.requestExecute (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:1008506)",
      "at d.requestExecute (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:37:325680)",
      "at w.requestExecute (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:32:18003)",
      "at w.executeCodeCell (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:301076)",
      "at w.execute (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:300703)",
      "at w.start (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:296367)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:311160)",
      "at async t.CellExecutionQueue.start (c:\\Users\\manash.jyoti.konwar\\.vscode\\extensions\\ms-toolsai.jupyter-2021.8.2031190714\\out\\client\\extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('chaiiQnAVenv': venv)"
  },
  "interpreter": {
   "hash": "b4a7a0862a028fbaf7a264e25ac7e1a433c43e5d0f2271def84b834f8b027919"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}